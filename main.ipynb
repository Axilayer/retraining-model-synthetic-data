{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e050ce-031c-429a-8a4e-910fffa17e60",
   "metadata": {},
   "source": [
    "## training and testing dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef28145-d0d6-4f30-9cbc-9eeee1e0fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dictionaries import generate_time_patterns, situations, symptoms, injury_causes, ambulance_equipment, location_of_occurrence, vital_signs, drugs, dosages, quantities, routes, sentence_templates\n",
    "from sentence_templates import generate_sentence \n",
    "\n",
    "label_counter = {\n",
    "    \"QUANTITY\": 0,\n",
    "    \"DOSAGE\": 0,\n",
    "    \"DRUG_NAME\": 0,\n",
    "    \"ROUTE\": 0,\n",
    "    \"TIME_OF_ACTION\": 0,\n",
    "    \"SITUATION\": 0,\n",
    "    \"SYMPTOM\": 0,\n",
    "    \"INJURY\": 0,\n",
    "    \"EQUIPMENT\": 0,\n",
    "    \"LOCATION\": 0,\n",
    "    \"VITAL_SIGN\": 0\n",
    "}\n",
    "TRAIN_DATA = []\n",
    "for _ in range(20): \n",
    "    # Randomly choose elements from the dictionaries\n",
    "    situation = random.choice(situations)\n",
    "    drug = random.choice(drugs)\n",
    "    dosage = random.choice(dosages)\n",
    "    quantity = random.choice(quantities)\n",
    "    route = random.choice(routes)\n",
    "    time_of_action = generate_time_patterns(1)\n",
    "    symptom = random.choice(symptoms)\n",
    "    injury_cause = random.choice(injury_causes)\n",
    "    equipment = random.choice(ambulance_equipment)\n",
    "    location = random.choice(location_of_occurrence)\n",
    "    vital_sign = random.choice(vital_signs)\n",
    "\n",
    "    sentence = generate_sentence(situation, drug, dosage, quantity, route, time_of_action, symptom, injury_cause, equipment, location, vital_sign)\n",
    "\n",
    "    #print(sentence)\n",
    "    entities = []\n",
    "    used_spans = set()\n",
    "    \n",
    "    def add_entity(label, value):\n",
    "        start = sentence.find(value)\n",
    "        end = start + len(value)\n",
    "        if start != -1 and (start, end) not in used_spans:\n",
    "            entities.append((start, end, label))\n",
    "            used_spans.add((start, end))\n",
    "    \n",
    "    add_entity(\"QUANTITY\", quantity)\n",
    "    add_entity(\"DOSAGE\", dosage)\n",
    "    add_entity(\"DRUG_NAME\", drug)\n",
    "    add_entity(\"ROUTE\", route)\n",
    "    add_entity(\"TIME_OF_ACTION\", time_of_action)\n",
    "    add_entity(\"SITUATION\", situation)\n",
    "    add_entity(\"SYMPTOM\", symptom)\n",
    "    add_entity(\"INJURY\", injury_cause)\n",
    "    add_entity(\"EQUIPMENT\", equipment)\n",
    "    add_entity(\"LOCATION\", location)\n",
    "    add_entity(\"VITAL_SIGN\", vital_sign)\n",
    "\n",
    "\n",
    "\n",
    "    for _, _, label in entities:\n",
    "        if label in label_counter:\n",
    "            label_counter[label] += 1\n",
    "    \n",
    "    TRAIN_DATA.append((sentence, {\"entities\": entities}))\n",
    "\n",
    "# Print the generated dataset\n",
    "#for entry in TRAIN_DATA:\n",
    "    #print(entry)\n",
    "    #print(entry[0])  \n",
    "\n",
    "with open(\"train_data.txt\", \"w\") as f:\n",
    "    for entry in TRAIN_DATA:\n",
    "        sentence = entry[0]  # The generated sentence\n",
    "        entities = entry[1][\"entities\"]  # The entities in the format of (start, end, label)\n",
    "        \n",
    "        # Write sentence and entities directly without additional labels\n",
    "        f.write(f\"{sentence}\\n\")\n",
    "        f.write(f\"{entities}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa9ea87-a854-452f-8800-08857094b46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 16\n",
      "Test data: 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training (80%) and test (20%)\n",
    "train_data, test_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data: {len(train_data)}\")\n",
    "print(f\"Test data: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8cdf6-be17-4f98-86c5-fc3bc92c16c5",
   "metadata": {},
   "source": [
    "## testing before retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23fae11-2cc7-4cb4-b34d-4c3094588b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: After The patient was found shivering and unresponsive due to hypothermia., Hematemesis was detected. The Blood pressure monitor in Public administrative building helped stabilize the patient while Heart rhythm was checked regularly.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "\n",
      "--- Ground Truth ---\n",
      "The patient was found shivering and unresponsive due to hypothermia. → SITUATION\n",
      "Hematemesis                    → SYMPTOM\n",
      "Blood pressure monitor         → EQUIPMENT\n",
      "Public administrative building → LOCATION\n",
      "Heart rhythm                   → VITAL_SIGN\n",
      "\n",
      "\n",
      "Text: While at the Recreation area, the EMS team delivered 5 sprays of Calcium Chloride / Lactate / Potassium Chloride / Sodium Chloride 10mg subcutaneous during appointment. A patient experiencing severe chest pain was assessed for possible myocardial infarction or aortic dissection.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "\n",
      "--- Ground Truth ---\n",
      "5 sprays                       → QUANTITY\n",
      "10mg                           → DOSAGE\n",
      "Calcium Chloride / Lactate / Potassium Chloride / Sodium Chloride → DRUG_NAME\n",
      "subcutaneous                   → ROUTE\n",
      "during appointment             → TIME_OF_ACTION\n",
      "A patient experiencing severe chest pain was assessed for possible myocardial infarction or aortic dissection. → SITUATION\n",
      "Recreation area                → LOCATION\n",
      "\n",
      "\n",
      "Text: Following protocol, the EMS team quickly gave 10mg of Adenosine 5mg using  subcutaneous. A patient with chronic anemia arrived feeling fatigued and short of breath, requiring further evaluation.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "\n",
      "--- Ground Truth ---\n",
      "10mg                           → QUANTITY\n",
      "5mg                            → DOSAGE\n",
      "Adenosine                      → DRUG_NAME\n",
      "subcutaneous                   → ROUTE\n",
      "A patient with chronic anemia arrived feeling fatigued and short of breath, requiring further evaluation. → SITUATION\n",
      "\n",
      "\n",
      "Text: After A motor vehicle accident left the patient in shock and requiring immediate care., the patient exhibited Pain in Chest. We used Nebulizer at Gas station to stabilize Respiratory effort and improve the condition.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "\n",
      "--- Ground Truth ---\n",
      "A motor vehicle accident left the patient in shock and requiring immediate care. → SITUATION\n",
      "Pain in Chest                  → SYMPTOM\n",
      "Nebulizer                      → EQUIPMENT\n",
      "Gas station                    → LOCATION\n",
      "Respiratory effort             → VITAL_SIGN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "import random\n",
    "\n",
    "# Initialize the blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "\n",
    "for text, annotations in test_data:\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Print the entities detected by the model\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"\\n--- Predicted Entities ---\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text:30} → {ent.label_}\")\n",
    "    \n",
    "    print(\"\\n--- Ground Truth ---\")\n",
    "    for start, end, label in annotations['entities']:\n",
    "        print(f\"{text[start:end]:30} → {label}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1a394-6d1d-43dc-9a8d-cc4405bcd0c9",
   "metadata": {},
   "source": [
    "## retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "046878da-e21f-4c39-af14-c80850234f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Aligned: 16 | ❌ Misaligned: 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "# Load your trained or base model (use 'en_core_web_sm' if unsure)\n",
    "nlp = spacy.blank(\"en\")  # or spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def check_entity_alignment(train_data):\n",
    "    aligned_data = []\n",
    "    misaligned_samples = []\n",
    "\n",
    "    for text, ann in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        try:\n",
    "            tags = offsets_to_biluo_tags(doc, ann[\"entities\"])\n",
    "            if \"-\" in tags:\n",
    "                print(f\"[MISALIGNED] -> '{text}'\")\n",
    "                print(f\"Entities: {ann['entities']}\\n\")\n",
    "                misaligned_samples.append((text, ann))\n",
    "            else:\n",
    "                aligned_data.append((text, ann))\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] -> '{text}' caused: {e}\")\n",
    "            misaligned_samples.append((text, ann))\n",
    "\n",
    "    print(f\"\\n✅ Aligned: {len(aligned_data)} | ❌ Misaligned: {len(misaligned_samples)}\")\n",
    "    return aligned_data, misaligned_samples\n",
    "\n",
    "# Run alignment check\n",
    "aligned, misaligned = check_entity_alignment(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2fabb947-83a7-43e4-8cf5-8ca00e196acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, annot in train_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    for start, end, label in annot[\"entities\"]:\n",
    "        span = doc.char_span(start, end, label=label)\n",
    "        if span is None:\n",
    "            print(f\"Misaligned: {text[start:end]} [{start}:{end}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b879f329-73ec-4e84-94f2-39ad90c7b49b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Losses: {'ner': 514.7792704105377}\n",
      "Epoch 1, Losses: {'ner': 223.17344353627107}\n",
      "Epoch 2, Losses: {'ner': 158.75673583450344}\n",
      "Epoch 3, Losses: {'ner': 180.2761879007569}\n",
      "Epoch 4, Losses: {'ner': 157.48548239551974}\n",
      "Epoch 5, Losses: {'ner': 173.57948865865762}\n",
      "Epoch 6, Losses: {'ner': 165.79927477186308}\n",
      "Epoch 7, Losses: {'ner': 192.41176317237745}\n",
      "Epoch 8, Losses: {'ner': 186.50564323110444}\n",
      "Epoch 9, Losses: {'ner': 188.79866420181634}\n",
      "Epoch 10, Losses: {'ner': 198.95356079624486}\n",
      "Epoch 11, Losses: {'ner': 146.048731084184}\n",
      "Epoch 12, Losses: {'ner': 159.13642855974297}\n",
      "Epoch 13, Losses: {'ner': 154.32410323300402}\n",
      "Epoch 14, Losses: {'ner': 138.7464992616022}\n",
      "Epoch 15, Losses: {'ner': 148.75618713417288}\n",
      "Epoch 16, Losses: {'ner': 132.73636298270995}\n",
      "Epoch 17, Losses: {'ner': 127.21273677925308}\n",
      "Epoch 18, Losses: {'ner': 129.774627314753}\n",
      "Epoch 19, Losses: {'ner': 121.0059259135441}\n"
     ]
    }
   ],
   "source": [
    "ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations[\"entities\"]:\n",
    "        ner.add_label(ent[2])\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):  # Adjust number of epochs based on your needs\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    \n",
    "    # Iterate through each example in the training data\n",
    "    for text, annotations in train_data:\n",
    "        # Create an Example object for each training sample\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "        nlp.update([example], drop=0.5, losses=losses)  # Drop used for regularization\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Losses: {losses}\")\n",
    "# Save the trained model to disk\n",
    "nlp.to_disk(\"custom_ner_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109364a9-9d18-452a-b4aa-b5b962a59568",
   "metadata": {},
   "source": [
    "## testing after retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e6ab88-ecc6-460d-a52a-0b9282ae9adb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "<spacy.scorer.Scorer object at 0x0000011F4C57E660>\n",
      "Evaluation Results:\n",
      "{'cats_auc_per_type': {},\n",
      " 'cats_f_per_type': {},\n",
      " 'cats_macro_auc': 0.0,\n",
      " 'cats_macro_f': 0.0,\n",
      " 'cats_macro_p': 0.0,\n",
      " 'cats_macro_r': 0.0,\n",
      " 'cats_micro_f': 0.0,\n",
      " 'cats_micro_p': 0.0,\n",
      " 'cats_micro_r': 0.0,\n",
      " 'cats_score': 0.0,\n",
      " 'cats_score_desc': 'macro F',\n",
      " 'dep_las': None,\n",
      " 'dep_las_per_type': None,\n",
      " 'dep_uas': None,\n",
      " 'ents_f': 0.0,\n",
      " 'ents_p': 0.0,\n",
      " 'ents_per_type': {'DOSAGE': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'DRUG_NAME': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'EQUIPMENT': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'LOCATION': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'QUANTITY': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'ROUTE': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'SITUATION': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'SYMPTOM': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'TIME_OF_ACTION': {'f': 0.0, 'p': 0.0, 'r': 0.0},\n",
      "                   'VITAL_SIGN': {'f': 0.0, 'p': 0.0, 'r': 0.0}},\n",
      " 'ents_r': 0.0,\n",
      " 'morph_acc': None,\n",
      " 'morph_micro_f': None,\n",
      " 'morph_micro_p': None,\n",
      " 'morph_micro_r': None,\n",
      " 'morph_per_feat': None,\n",
      " 'pos_acc': None,\n",
      " 'sents_f': None,\n",
      " 'sents_p': None,\n",
      " 'sents_r': None,\n",
      " 'tag_acc': None,\n",
      " 'token_acc': 1.0,\n",
      " 'token_f': 1.0,\n",
      " 'token_p': 1.0,\n",
      " 'token_r': 1.0}\n",
      "Text: After The patient was found shivering and unresponsive due to hypothermia., Hematemesis was detected. The Blood pressure monitor in Public administrative building helped stabilize the patient while Heart rhythm was checked regularly.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "The patient was found shivering and unresponsive due to hypothermia. → SITUATION\n",
      "Hematemesis was detected       → LOCATION\n",
      "Blood pressure                 → EQUIPMENT\n",
      "Public administrative building → LOCATION\n",
      "Heart rhythm was               → LOCATION\n",
      "\n",
      "--- Ground Truth ---\n",
      "The patient was found shivering and unresponsive due to hypothermia. → SITUATION\n",
      "Hematemesis                    → SYMPTOM\n",
      "Blood pressure monitor         → EQUIPMENT\n",
      "Public administrative building → LOCATION\n",
      "Heart rhythm                   → VITAL_SIGN\n",
      "\n",
      "\n",
      "Text: While at the Recreation area, the EMS team delivered 5 sprays of Calcium Chloride / Lactate / Potassium Chloride / Sodium Chloride 10mg subcutaneous during appointment. A patient experiencing severe chest pain was assessed for possible myocardial infarction or aortic dissection.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "While at                       → EQUIPMENT\n",
      "Recreation area                → EQUIPMENT\n",
      "5 sprays                       → QUANTITY\n",
      "Calcium                        → DRUG_NAME\n",
      "Chloride /                     → DOSAGE\n",
      "Lactate                        → DRUG_NAME\n",
      "/ Potassium                    → DOSAGE\n",
      "Chloride /                     → DOSAGE\n",
      "Sodium                         → DRUG_NAME\n",
      "Chloride                       → DRUG_NAME\n",
      "10mg                           → DOSAGE\n",
      "subcutaneous                   → ROUTE\n",
      "A patient experiencing severe chest pain was assessed for possible myocardial infarction or aortic dissection. → SITUATION\n",
      "\n",
      "--- Ground Truth ---\n",
      "5 sprays                       → QUANTITY\n",
      "10mg                           → DOSAGE\n",
      "Calcium Chloride / Lactate / Potassium Chloride / Sodium Chloride → DRUG_NAME\n",
      "subcutaneous                   → ROUTE\n",
      "during appointment             → TIME_OF_ACTION\n",
      "A patient experiencing severe chest pain was assessed for possible myocardial infarction or aortic dissection. → SITUATION\n",
      "Recreation area                → LOCATION\n",
      "\n",
      "\n",
      "Text: Following protocol, the EMS team quickly gave 10mg of Adenosine 5mg using  subcutaneous. A patient with chronic anemia arrived feeling fatigued and short of breath, requiring further evaluation.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "10mg                           → QUANTITY\n",
      "Adenosine                      → DRUG_NAME\n",
      "5mg                            → DOSAGE\n",
      "A patient with chronic anemia arrived feeling fatigued and short of breath, requiring further evaluation. → SITUATION\n",
      "\n",
      "--- Ground Truth ---\n",
      "10mg                           → QUANTITY\n",
      "5mg                            → DOSAGE\n",
      "Adenosine                      → DRUG_NAME\n",
      "subcutaneous                   → ROUTE\n",
      "A patient with chronic anemia arrived feeling fatigued and short of breath, requiring further evaluation. → SITUATION\n",
      "\n",
      "\n",
      "Text: After A motor vehicle accident left the patient in shock and requiring immediate care., the patient exhibited Pain in Chest. We used Nebulizer at Gas station to stabilize Respiratory effort and improve the condition.\n",
      "\n",
      "--- Predicted Entities ---\n",
      "A motor vehicle accident left the patient in shock and requiring immediate care. → SITUATION\n",
      "Pain in                        → VITAL_SIGN\n",
      "Chest. We                      → LOCATION\n",
      "Nebulizer at                   → EQUIPMENT\n",
      "Respiratory effort             → EQUIPMENT\n",
      "\n",
      "--- Ground Truth ---\n",
      "A motor vehicle accident left the patient in shock and requiring immediate care. → SITUATION\n",
      "Pain in Chest                  → SYMPTOM\n",
      "Nebulizer                      → EQUIPMENT\n",
      "Gas station                    → LOCATION\n",
      "Respiratory effort             → VITAL_SIGN\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import scorer\n",
    "nlp = spacy.load(\"custom_ner_model\")\n",
    "\n",
    "sc = scorer.Scorer()\n",
    "\n",
    "examples = []  \n",
    "\n",
    "for text, annotations in test_data:\n",
    "    doc = nlp.make_doc(text)  \n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "scores = sc.score(examples)\n",
    "\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(sc)\n",
    "\n",
    "# Print the results\n",
    "from pprint import pprint\n",
    "print(\"Evaluation Results:\")\n",
    "pprint(scores)\n",
    "\n",
    "for text, annotations in test_data:\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Print the entities detected by the model\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"\\n--- Predicted Entities ---\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text:30} → {ent.label_}\")\n",
    "    \n",
    "    print(\"\\n--- Ground Truth ---\")\n",
    "    for start, end, label in annotations['entities']:\n",
    "        print(f\"{text[start:end]:30} → {label}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5682a0-4c10-4f0c-ab35-15f0b6d65f18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
